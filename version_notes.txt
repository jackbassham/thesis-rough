VERSION 01072026_164301
RUN:
Ran all scripts for both SH and NH. CNN in both hempsipheres and WTD CNN in SH produced expected skills.
WTD CNN in NH was noisy.
LR and WTD LR in both SH and NH had sparse outputs (singular matrix, NaNs at many locations).
FIX: 
Had switched to assigining present day/ previous day variables after masking/ normalizing in this version.
Switching back to masking/ normalizing after assigning variables fixed sparse LR outputs in proceding version.

VERSION 01162026_164705
RUN:
Ran from mask-norm on for both SH and NH with fixes mentioned directly above. Expected skills.
Still encountering -inf skills in both hemispheres
(probably due to close to zero variance in true values giving zero division).
These points seem to be beyond the ice edge.
DEBUG:
Playing with ice concentration mask used in Hoffman et. al. based on percent time ice free (in addition to NSIDC mask, mask
locations where ice cocentration = 0 for > 90% of the time (v.s. 80 for Hoffman)).
The 90% mask captures points in SH with near zero variance with less area removed from domain than others (visually). Visual comparison
with negative skill.
Trying SH and NH with 90% threshold for ice free days.
Trying issue with
FIX: 
None yet - improvement but still seeing issues with skill.
NEW PROBLEM:
WTD LR for SH and NH maxed out negative skill and WTD CNN for NH showing noise - compare uncertainty stats for both hemispheres.

VERSION 01222026_1511
RUN:
Ran plots only with small epsilon correction in skill and weighted skill (1e-4). Still seeing large negative skill values.
Continue with masking.
DEBUG:
Added epsilon value for skills (had it previously, forgot to inlude it in these versions).

VERSION 01232026_1104
RUN:
Ran only lr_wtd_cf for NH and SH. Unique output for y_pred and y_true (test output)! Skill looks valid in both SH and NH! 
FIX:
Problem was likely memory allocation with np.full for y_pred and setting y_true = y_pred.

VERSION 01232026_1338
RUN:
Ran both SH and NH from mask-norm on with mask at spatial points with 80% days ice free threshold
Looks better! Still not perfect. Think about combining variance mask with
mask based on 80% or 90% threshold.

VERSION 01252026_1525
RUN:
Ran both SH and NH from mask-norm on with mask at spatial points with < 0.05 ice velocity
variance threshold.

Did Not see imporovement.

...

VERSION 02132026_1142
RUN:
Ran both SH and NH from mask-norm on with mask at:
1. Entire spatial grid points where ice concentration is less than or equal to 0.15 
more than 70% of the time
2. Entire spatial grid points where Ice concentration, zonal, or zeridional ice velocity are NaN.
3. Single data points where ice concentration is less than or equal to 0.15

Seeing reasonable skill, outliers reduced 
- seems to be a sweet spot for the mask. KEEPING FOR NOW.

* Moving forward to debugging weighted CNN outputs from NH, bad uncertainty.

VERSION 02132026_0924
FIX:
Found NH contains negative values uncertainty (for vectors close to the coast) 
while SH dataset does not. The spread in uncertainty and negative uncertainty
seemed to be causing instability in Weighted CNN Weighted NRMSE loss function.

Moved to using absolute value of uncertainty. (r = abs(r))

RUN:
Ran NH dataset from mask-norm on, using absolute value of uncertainty.

NH output now showing reasonable skill for weighted CNN!

VERSION 02132026_1142
Playing arround with bypassing mask on wind in mask-norm so that the 
CNN trains using wind over land and missing points. 

RUN: Ran both SH and NH from mask norm on using unmasked wind inputs. Saw a very slight reduction
in skill (~ -0.02) overall. Nothing super noteworthy.

REVERTING TO masking wind.

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Moving Forward: git branch 'ensemble':
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Improvements before actually implementing an ensemble:
1. Consolidate LR and CNN inputs (avoid repetitive preprocessing), 
using LR masked inputs and converting NaN to zero and inputs 
to pytorch tensors before running CNN.

2. Clean up workflow and paths:
- Clean up file names - remove redundancy from filenames, keep 
directory structure
- * Move toward python pipleline after research steps
- Implement a python cofig.py module for global parameters
- Consolidate param and path to one script?
- Use python script for orchestration, but can keep bash script
for running ('HPC?') for now?


3. Try to include mask as a CNN input and a masked loss function!
So that CNN learns only from real data!

...

...

4. Then, when running smoothly, move to implementing an ensemble
with 10 members; each consisting of randomized block of 2 test years, 
block of 2 val years, and the rest train years
* aka 'Blocked Monte Carlo' ensemble approach



