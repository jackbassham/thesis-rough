{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8d13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO why difference between mask/norm before day shift and mask/norm after?\n",
    "# NOTE\n",
    "# 1. Number of nan points are similar\n",
    "# 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce7a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# # # WITHOUT DAY SHIFT FIRST\n",
    "# # #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# # Load in raw regrid data\n",
    "# fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/motion_ppv4_latlon_sh19922020_20260107_164301.npz\"\n",
    "# data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "# ui = data['u'] # zonal ice velocity\n",
    "# vi = data['v'] # meridional ice velocity\n",
    "# ri = data['r'] # ice velocity uncertainty (same for u and v)\n",
    "\n",
    "# print('Ice Velocity, Uncertainty Loaded')\n",
    "\n",
    "# fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/con_nimbus7_latlon_sh19922020_20260107_164301.npz\"\n",
    "# data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "# ci = data['ci']# ice concentration\n",
    "\n",
    "# print('Concentration Loaded')\n",
    "\n",
    "# fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/wind_jra55_latlon_sh19922020_20260107_164301.npz\"\n",
    "# data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "# ua = data['u']\n",
    "# va = data['v']\n",
    "\n",
    "# print('Wind Loaded')\n",
    "\n",
    "# # Delete 'data' from memory\n",
    "# del data\n",
    "# gc.collect()\n",
    "\n",
    "# print('Variable Files Loaded')\n",
    "# print('')\n",
    "\n",
    "# # Mask ice concentration\n",
    "# ci_raw = np.round(ci * 250) # raw value ice concentration (NSIDC)\n",
    "\n",
    "# # NSIDC Masks \n",
    "# # 251 pole hole\n",
    "# # 252 unused data\n",
    "# # 253 coastline\n",
    "# # 254 land\n",
    "# ci = np.where((ci_raw == 251) | (ci_raw == 252) | (ci_raw == 253) | (ci_raw == 254), np.nan, ci)\n",
    "\n",
    "# print('Raw concentration masked based on NSIDC masks.')\n",
    "\n",
    "# # Create list of input variables\n",
    "# invars = [ui, vi, ri, ua, va, ci]\n",
    "\n",
    "# # Mask spatial indices with concentration less than .15, NaN concentration\n",
    "# # NOTE keeping flag values for ice velocity uncertainties\n",
    "# mask = (ci <= .15) | (np.isnan(ci))\n",
    "\n",
    "# # NaN out points meeting mask condition\n",
    "# invars_masked = [np.where(mask, np.nan, var) for var in invars]\n",
    "\n",
    "# print('Inputs masked where ice concentration values <= .15 or nan.')\n",
    "# print('')\n",
    "\n",
    "# # NOTE: Normalization (z-score, for comparison between variables - 0 mean, 1 std)\n",
    "# # 1. Compute temporal mean, gridwise\n",
    "# # 2. Compute global standard deviation\n",
    "# # 3. Remove mean and divde by standard deviation\n",
    "# # 4. ** Ice velocities here are normalized by the standard deviation of the speed\n",
    "# # 5. ** Uncertainty here is scaled by ci_std\n",
    "\n",
    "# # Compute temporal mean of inputs at every gridpoint\n",
    "# grid_means = [np.nanmean(var, axis = 0) for var in invars_masked]\n",
    "\n",
    "# # Compute global stds\n",
    "# global_stds = [np.nanstd(var) for var in invars_masked]\n",
    "\n",
    "# # Unpacked masked variables\n",
    "# ui_masked, vi_masked, ri_masked, ua_masked, va_masked, ci_masked = invars_masked\n",
    "\n",
    "# # Delete unused arrays from memory\n",
    "# del invars\n",
    "# del ui, vi, ri, ua, va, ci\n",
    "# gc.collect()\n",
    "\n",
    "# # Unpack statistics\n",
    "# ui_bar, vi_bar, _, ua_bar, va_bar, ci_bar = grid_means\n",
    "\n",
    "# _, _, _, ua_std, va_std, ci_std = global_stds\n",
    "\n",
    "# # Delete unused arrays from memory\n",
    "# del _\n",
    "# gc.collect()\n",
    "\n",
    "# # Calculate speed\n",
    "# Ui = np.sqrt(ui_masked ** 2 + vi_masked ** 2)\n",
    "\n",
    "# # Get standard deviation of speed for normalization\n",
    "# Ui_std = np.nanstd(Ui)\n",
    "\n",
    "# # Delete unused arrays from memory\n",
    "# del invars_masked, grid_means, global_stds\n",
    "# gc.collect()\n",
    "\n",
    "# # Normalize ice velocity and uncertainty by ice speed global standard deviation \n",
    "# # (z-score normalization)\n",
    "\n",
    "# ui_norm = (ui_masked - ui_bar) / Ui_std\n",
    "# vi_norm = (vi_masked - vi_bar) / Ui_std\n",
    "\n",
    "# print(\"'uit_bar', 'vit_bar' normalized by 'cit_std:'\")\n",
    "# print(f\"   {Ui_std:.3f} cm/s\")\n",
    "# print('')\n",
    "\n",
    "# # Normalize uncertainty by standard deviation of speed\n",
    "# ri_norm = ri_masked / Ui_std\n",
    "\n",
    "# print(f\"'rt' scaled by {Ui_std:.3f} cm/s:\")\n",
    "# print('')\n",
    "\n",
    "# # Normalize remaning variables\n",
    "# ua_norm = (ua_masked - ua_bar) / ua_std\n",
    "\n",
    "# va_norm = (va_masked - va_bar) / va_std\n",
    "\n",
    "# ci_norm = (ci_masked - ci_bar) / ci_std\n",
    "\n",
    "# print(\"'ua', 'va', and 'ci' normalized by respective standard devations:\")\n",
    "# print(f\"   {ua_std:.3f} cm/s, {va_std:.3f} cm/s, {ci_std:.3f}\")\n",
    "# print('')\n",
    "\n",
    "# # Pack normalized input variables into list\n",
    "# invars_norm = [ui_norm, vi_norm, ri_norm, ua_norm, va_norm, ci_norm]\n",
    "\n",
    "# # Count number of data points in each variable\n",
    "# total_points = [var.size for var in invars_norm]\n",
    "\n",
    "# # Count the number of nans in each variable\n",
    "# total_nan = [np.isnan(var).sum() for var in invars_norm]\n",
    "\n",
    "# for p, n in zip(total_points, total_nan):\n",
    "#     print(f\"total points/ total nan: {p} / {n}\")\n",
    "#     print(f\"num valid points {p - n}\")\n",
    "#     print(f\"frac nan {p / n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ice Velocity, Uncertainty Loaded\n",
      "Concentration Loaded\n",
      "Wind Loaded\n",
      "Variable Files Loaded\n",
      "\n",
      "Raw concentration masked based on NSIDC masks.\n",
      "Inputs masked where ice concentration values <= .15 or nan.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114563/120357705.py:83: RuntimeWarning: Mean of empty slice\n",
      "  grid_means = [np.nanmean(var, axis = 0) for var in invars_masked]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'uit_bar', 'vit_bar' normalized by 'cit_std:'\n",
      "   7.937 cm/s\n",
      "\n",
      "'rt' scaled by 7.937 cm/s:\n",
      "\n",
      "'ua', 'va', and 'ci' normalized by respective standard devations:\n",
      "   6.360 cm/s, 5.161 cm/s, 0.197\n",
      "\n",
      "total points/ total nan: 447850944 / 305858109\n",
      "num valid points 141992835\n",
      "frac nan 1.4642441407365139\n",
      "total points/ total nan: 447850944 / 305858109\n",
      "num valid points 141992835\n",
      "frac nan 1.4642441407365139\n",
      "total points/ total nan: 447850944 / 305858109\n",
      "num valid points 141992835\n",
      "frac nan 1.4642441407365139\n",
      "total points/ total nan: 447850944 / 293633495\n",
      "num valid points 154217449\n",
      "frac nan 1.5252038736248397\n",
      "total points/ total nan: 447850944 / 293633495\n",
      "num valid points 154217449\n",
      "frac nan 1.5252038736248397\n",
      "total points/ total nan: 447850944 / 293633495\n",
      "num valid points 154217449\n",
      "frac nan 1.5252038736248397\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# WITH DAY SHIFT FIRST\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# Load in raw regrid data\n",
    "fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/motion_ppv4_latlon_sh19922020_20260107_164301.npz\"\n",
    "data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "ui = data['u'] # zonal ice velocity\n",
    "vi = data['v'] # meridional ice velocity\n",
    "ri = data['r'] # ice velocity uncertainty (same for u and v)\n",
    "\n",
    "print('Ice Velocity, Uncertainty Loaded')\n",
    "\n",
    "fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/con_nimbus7_latlon_sh19922020_20260107_164301.npz\"\n",
    "data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "ci = data['ci']# ice concentration\n",
    "\n",
    "print('Concentration Loaded')\n",
    "\n",
    "fnam = \"/home/jbassham/jack/thesis-rough/data/regrid/sh/20260107_164301/wind_jra55_latlon_sh19922020_20260107_164301.npz\"\n",
    "data = np.load(fnam, allow_pickle=True)\n",
    "\n",
    "ua = data['u']\n",
    "va = data['v']\n",
    "\n",
    "print('Wind Loaded')\n",
    "\n",
    "# Delete 'data' from memory\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "print('Variable Files Loaded')\n",
    "print('')\n",
    "\n",
    "# Mask ice concentration\n",
    "ci_raw = np.round(ci * 250) # raw value ice concentration (NSIDC)\n",
    "\n",
    "# NSIDC Masks \n",
    "# 251 pole hole\n",
    "# 252 unused data\n",
    "# 253 coastline\n",
    "# 254 land\n",
    "ci = np.where((ci_raw == 251) | (ci_raw == 252) | (ci_raw == 253) | (ci_raw == 254), np.nan, ci)\n",
    "\n",
    "print('Raw concentration masked based on NSIDC masks.')\n",
    "\n",
    "# Shift present day parameters forward one day, for one point Middle Weddell\n",
    "ui_t0 = ui[1:,:,:]\n",
    "vi_t0 = vi[1:,:,:]\n",
    "ua_t0 = ua[1:,:,:]\n",
    "va_t0 = va[1:,:,:]\n",
    "ri_t0 = ri[1:,:,:]\n",
    "\n",
    "# Get present day ice concentration for masking\n",
    "ci_t0 = ci[1:,:,:]\n",
    "\n",
    "# Remove last day from previous day parameters\n",
    "ci_t1 = ci[:-1,:,:]\n",
    "\n",
    "# Create list of input variables\n",
    "invars = [ui_t0, vi_t0, ri_t0, ua_t0, va_t0, ci_t1]\n",
    "\n",
    "# Mask spatial indices with concentration less than .15, NaN concentration\n",
    "# NOTE keeping flag values for ice velocity uncertainties\n",
    "mask = (ci_t0 <= .15) | (np.isnan(ci_t0))\n",
    "\n",
    "# NaN out points meeting mask condition\n",
    "invars_masked = [np.where(mask, np.nan, var) for var in invars]\n",
    "\n",
    "print('Inputs masked where ice concentration values <= .15 or nan.')\n",
    "print('')\n",
    "\n",
    "# NOTE: Normalization (z-score, for comparison between variables - 0 mean, 1 std)\n",
    "# 1. Compute temporal mean, gridwise\n",
    "# 2. Compute global standard deviation\n",
    "# 3. Remove mean and divde by standard deviation\n",
    "# 4. ** Ice velocities here are normalized by the standard deviation of the speed\n",
    "# 5. ** Uncertainty here is scaled by ci_std\n",
    "\n",
    "# Compute temporal mean of inputs at every gridpoint\n",
    "grid_means = [np.nanmean(var, axis = 0) for var in invars_masked]\n",
    "\n",
    "# Compute global stds\n",
    "global_stds = [np.nanstd(var) for var in invars_masked]\n",
    "\n",
    "# Unpacked masked variables\n",
    "ui_masked, vi_masked, ri_masked, ua_masked, va_masked, ci_masked = invars_masked\n",
    "\n",
    "# Delete unused arrays from memory\n",
    "del invars\n",
    "del ui, vi, ri, ua, va, ci\n",
    "gc.collect()\n",
    "\n",
    "# Unpack statistics\n",
    "ui_bar, vi_bar, _, ua_bar, va_bar, ci_bar = grid_means\n",
    "\n",
    "_, _, _, ua_std, va_std, ci_std = global_stds\n",
    "\n",
    "# Delete unused arrays from memory\n",
    "del _\n",
    "gc.collect()\n",
    "\n",
    "# Calculate speed\n",
    "Ui = np.sqrt(ui_masked ** 2 + vi_masked ** 2)\n",
    "\n",
    "# Get standard deviation of speed for normalization\n",
    "Ui_std = np.nanstd(Ui)\n",
    "\n",
    "# Delete unused arrays from memory\n",
    "del invars_masked, grid_means, global_stds\n",
    "gc.collect()\n",
    "\n",
    "# Normalize ice velocity and uncertainty by ice speed global standard deviation \n",
    "# (z-score normalization)\n",
    "\n",
    "ui_norm = (ui_masked - ui_bar) / Ui_std\n",
    "vi_norm = (vi_masked - vi_bar) / Ui_std\n",
    "\n",
    "print(\"'uit_bar', 'vit_bar' normalized by 'cit_std:'\")\n",
    "print(f\"   {Ui_std:.3f} cm/s\")\n",
    "print('')\n",
    "\n",
    "# Normalize uncertainty by standard deviation of speed\n",
    "ri_norm = ri_masked / Ui_std\n",
    "\n",
    "print(f\"'rt' scaled by {Ui_std:.3f} cm/s:\")\n",
    "print('')\n",
    "\n",
    "# Normalize remaning variables\n",
    "ua_norm = (ua_masked - ua_bar) / ua_std\n",
    "\n",
    "va_norm = (va_masked - va_bar) / va_std\n",
    "\n",
    "ci_norm = (ci_masked - ci_bar) / ci_std\n",
    "\n",
    "print(\"'ua', 'va', and 'ci' normalized by respective standard devations:\")\n",
    "print(f\"   {ua_std:.3f} cm/s, {va_std:.3f} cm/s, {ci_std:.3f}\")\n",
    "print('')\n",
    "\n",
    "# Pack normalized input variables into list\n",
    "invars_norm = [ui_norm, vi_norm, ri_norm, ua_norm, va_norm, ci_norm]\n",
    "\n",
    "# Count number of data points in each variable\n",
    "total_points = [var.size for var in invars_norm]\n",
    "\n",
    "# Count the number of nans in each variable\n",
    "total_nan = [np.isnan(var).sum() for var in invars_norm]\n",
    "\n",
    "for p, n in zip(total_points, total_nan):\n",
    "    print(f\"total points/ total nan: {p} / {n}\")\n",
    "    print(f\"num valid points {p - n}\")\n",
    "    print(f\"frac nan (invalid) {n / p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060ae78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature and Target Arrays filled\n"
     ]
    }
   ],
   "source": [
    "# Define number of input channels\n",
    "n_in = 3\n",
    "\n",
    "# Define number of output channels\n",
    "n_out = 2\n",
    "\n",
    "# Get data dimensions\n",
    "nt, nlat, nlon = np.shape(ui_t0) # time, latitude, longitude\n",
    "\n",
    "# Initialize feature and target arrays (batch, channels, height, width)\n",
    "x = np.zeros((nt, n_in, nlat, nlon)) # Features\n",
    "y = np.zeros((nt, n_out, nlat, nlon)) # Targets\n",
    "\n",
    "# Fill feature arrays\n",
    "x[:, 0, :, :] = ua_norm # Zonal Wind, present day\n",
    "x[:, 1, :, :] = va_norm # Meridional Wind, present day\n",
    "x[:, 2, :, :] = ci_norm # Ice Concentration, previous day\n",
    "\n",
    "# Fill target arrays\n",
    "y[:, 0, :, :] = ui_norm # Zonal Ice Velocity, present day\n",
    "y[:, 1, :, :] = vi_norm # Meridional Ice Velocity, present day\n",
    "\n",
    "print(\"Feature and Target Arrays filled\")\n",
    "\n",
    "# Reshape uncertainty\n",
    "# NOTE DO NOT reshape uncertainty for LR\n",
    "# ri_t0 = np.expand_dims(ri_t0, 1)\n",
    "# # ri_t0 = ri_t0.unsqueeze(1) # [nt, 1, nlat, nlon]\n",
    "\n",
    "# Extract time (dates)\n",
    "fnam = \"/home/jbassham/jack/thesis-rough/data/coordinates/sh/20260107_164301/coord_sh19922020_20260107_164301.npz\"\n",
    "data = np.load(fnam, allow_pickle=True)\n",
    "time = data['time']\n",
    "\n",
    "# Create present day (t0) time coordinate variable by shifting forward one day\n",
    "time_t0 = time[1:]\n",
    "\n",
    "years = time_t0.astype('datetime64[Y]').astype(int) + 1970\n",
    "\n",
    "# Define split mask based on years\n",
    "train_mask = (years >= 1992) & (years <= 2016)\n",
    "# val_mask   = (years >= 2017) & (years <= 2018)\n",
    "test_mask  = (years >= 2019) & (years <= 2020)\n",
    "\n",
    "# Get split indices\n",
    "train_idx = np.where(train_mask)[0]\n",
    "# val_idx   = np.where(val_mask)[0]\n",
    "test_idx  = np.where(test_mask)[0]\n",
    "\n",
    "# Fill train, validation, and test data arrays\n",
    "x_train, y_train, r_train = x[train_idx], y[train_idx], ri_norm[train_idx]\n",
    "# x_val, y_val, r_val = x[val_idx], y[val_idx], ri_t0[val_idx]\n",
    "x_test, y_test, r_test = x[test_idx], y[test_idx], ri_norm[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0f6921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits\n",
    "fnam = 'new_exp_train_inputs.npz'\n",
    "\n",
    "np.savez(\n",
    "    fnam,\n",
    "    x_train = x_train,\n",
    "    y_train = y_train,\n",
    "    r_train = r_train\n",
    ")\n",
    "\n",
    "# Save splits\n",
    "fnam = 'new_exp_test_inputs.npz'\n",
    "\n",
    "np.savez(\n",
    "    fnam,\n",
    "    x_test = x_test,\n",
    "    y_test = y_test,\n",
    "    r_test = r_test\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seaice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
