{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "606aa254-d549-4b8e-a1a2-8d52695817f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr # With h5netcdf dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b795051c-3458-4172-8f94-6eed173da157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrids NSIDC Polar Pathfinder Sea Ice Velocities from EASE to regular lat lon coordinate system\n",
    "# NOTE See Southern Hemisphere vs Northern Hemisphere vector rotation below\n",
    "\n",
    "# 04SEP24 Northern Hemisphere Version\n",
    "\n",
    "# Enter data source path\n",
    "PATH_SOURCE = \"/home/jbassham/north/data/motion\"\n",
    "# Enter file name (end of URL) with placeholder {year}\n",
    "FNAM = \"icemotion_daily_nh_25km_{year}0101_{year}1231_v4.1.nc\"\n",
    "\n",
    "# Enter destination path\n",
    "PATH_DEST = \"/home/jbassham/north/data\"\n",
    "\n",
    "# Enter years to regrid\n",
    "START_YEAR = 2020\n",
    "END_YEAR = 2020\n",
    "\n",
    "# Enter bounds for lat and lon\n",
    "LAT_LIM = [60, 90] \n",
    "LON_LIM = [-180, 180]\n",
    "\n",
    "# Enter new grid resolution (converting 25 km to degrees latitude)\n",
    "YRES = 25 / 111   # Degrees latitude at 25km resolution\n",
    "XRES = 25 / (111*np.cos(np.abs((LAT_LIM[0]+LAT_LIM[1])/2))) # Degrees longitude at 25km resolution (based on average latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf2c40a1-5f47-4a14-ab55-1ff58692488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lon 0 complete\n",
      "lon 1 complete\n",
      "lon 2 complete\n",
      "lon 3 complete\n",
      "lon 4 complete\n",
      "lon 5 complete\n",
      "lon 6 complete\n",
      "lon 7 complete\n",
      "lon 8 complete\n",
      "lon 9 complete\n",
      "lon 10 complete\n",
      "lon 11 complete\n",
      "lon 12 complete\n",
      "lon 13 complete\n",
      "lon 14 complete\n",
      "lon 15 complete\n",
      "lon 16 complete\n",
      "lon 17 complete\n",
      "lon 18 complete\n",
      "lon 19 complete\n",
      "lon 20 complete\n",
      "lon 21 complete\n",
      "lon 22 complete\n",
      "lon 23 complete\n",
      "lon 24 complete\n",
      "lon 25 complete\n",
      "lon 26 complete\n",
      "lon 27 complete\n",
      "lon 28 complete\n",
      "lon 29 complete\n",
      "lon 30 complete\n",
      "lon 31 complete\n",
      "lon 32 complete\n",
      "lon 33 complete\n",
      "lon 34 complete\n",
      "lon 35 complete\n",
      "lon 36 complete\n",
      "lon 37 complete\n",
      "lon 38 complete\n",
      "lon 39 complete\n",
      "lon 40 complete\n",
      "lon 41 complete\n",
      "lon 42 complete\n",
      "lon 43 complete\n",
      "lon 44 complete\n",
      "lon 45 complete\n",
      "lon 46 complete\n",
      "lon 47 complete\n",
      "lon 48 complete\n",
      "lon 49 complete\n",
      "lon 50 complete\n",
      "lon 51 complete\n",
      "lon 52 complete\n",
      "lon 53 complete\n",
      "lon 54 complete\n",
      "lon 55 complete\n",
      "lon 56 complete\n",
      "lon 57 complete\n",
      "lon 58 complete\n",
      "lon 59 complete\n",
      "lon 60 complete\n",
      "lon 61 complete\n",
      "lon 62 complete\n",
      "lon 63 complete\n",
      "lon 64 complete\n",
      "lon 65 complete\n",
      "lon 66 complete\n",
      "lon 67 complete\n",
      "lon 68 complete\n",
      "lon 69 complete\n",
      "lon 70 complete\n",
      "lon 71 complete\n",
      "lon 72 complete\n",
      "lon 73 complete\n",
      "lon 74 complete\n",
      "lon 75 complete\n",
      "lon 76 complete\n",
      "lon 77 complete\n",
      "lon 78 complete\n",
      "lon 79 complete\n",
      "lon 80 complete\n",
      "lon 81 complete\n",
      "lon 82 complete\n",
      "lon 83 complete\n",
      "lon 84 complete\n",
      "lon 85 complete\n",
      "lon 86 complete\n",
      "lon 87 complete\n",
      "lon 88 complete\n",
      "lon 89 complete\n",
      "lon 90 complete\n",
      "lon 91 complete\n",
      "lon 92 complete\n",
      "lon 93 complete\n",
      "lon 94 complete\n",
      "lon 95 complete\n",
      "lon 96 complete\n",
      "lon 97 complete\n",
      "lon 98 complete\n",
      "lon 99 complete\n",
      "lon 100 complete\n",
      "lon 101 complete\n",
      "lon 102 complete\n",
      "lon 103 complete\n",
      "lon 104 complete\n",
      "lon 105 complete\n",
      "lon 106 complete\n",
      "lon 107 complete\n",
      "lon 108 complete\n",
      "lon 109 complete\n",
      "lon 110 complete\n",
      "lon 111 complete\n",
      "lon 112 complete\n",
      "lon 113 complete\n",
      "lon 114 complete\n",
      "lon 115 complete\n",
      "lon 116 complete\n",
      "lon 117 complete\n",
      "lon 118 complete\n",
      "lon 119 complete\n",
      "lon 120 complete\n",
      "lon 121 complete\n",
      "lon 122 complete\n",
      "lon 123 complete\n",
      "lon 124 complete\n",
      "lon 125 complete\n",
      "lon 126 complete\n",
      "lon 127 complete\n",
      "lon 128 complete\n",
      "lon 129 complete\n",
      "lon 130 complete\n",
      "lon 131 complete\n",
      "lon 132 complete\n",
      "lon 133 complete\n",
      "lon 134 complete\n",
      "lon 135 complete\n",
      "lon 136 complete\n",
      "lon 137 complete\n",
      "lon 138 complete\n",
      "lon 139 complete\n",
      "lon 140 complete\n",
      "lon 141 complete\n",
      "lon 142 complete\n",
      "lon 143 complete\n",
      "lon 144 complete\n",
      "lon 145 complete\n",
      "lon 146 complete\n",
      "lon 147 complete\n",
      "lon 148 complete\n",
      "lon 149 complete\n",
      "lon 150 complete\n",
      "lon 151 complete\n",
      "lon 152 complete\n",
      "lon 153 complete\n",
      "lon 154 complete\n",
      "lon 155 complete\n",
      "lon 156 complete\n",
      "lon 157 complete\n",
      "lon 158 complete\n",
      "lon 159 complete\n",
      "lon 160 complete\n",
      "lon 161 complete\n",
      "lon 162 complete\n",
      "lon 163 complete\n",
      "lon 164 complete\n",
      "lon 165 complete\n",
      "lon 166 complete\n",
      "lon 167 complete\n",
      "lon 168 complete\n",
      "lon 169 complete\n",
      "lon 170 complete\n",
      "lon 171 complete\n",
      "lon 172 complete\n",
      "lon 173 complete\n",
      "lon 174 complete\n",
      "lon 175 complete\n",
      "lon 176 complete\n",
      "lon 177 complete\n",
      "lon 178 complete\n",
      "lon 179 complete\n",
      "lon 180 complete\n",
      "lon 181 complete\n",
      "lon 182 complete\n",
      "lon 183 complete\n",
      "lon 184 complete\n",
      "lon 185 complete\n",
      "lon 186 complete\n",
      "lon 187 complete\n",
      "lon 188 complete\n",
      "lon 189 complete\n",
      "lon 190 complete\n",
      "lon 191 complete\n",
      "lon 192 complete\n",
      "lon 193 complete\n",
      "lon 194 complete\n",
      "lon 195 complete\n",
      "lon 196 complete\n",
      "lon 197 complete\n",
      "lon 198 complete\n",
      "lon 199 complete\n",
      "lon 200 complete\n",
      "lon 201 complete\n",
      "lon 202 complete\n",
      "lon 203 complete\n",
      "lon 204 complete\n",
      "lon 205 complete\n",
      "lon 206 complete\n",
      "lon 207 complete\n",
      "lon 208 complete\n",
      "lon 209 complete\n",
      "lon 210 complete\n",
      "lon 211 complete\n",
      "lon 212 complete\n",
      "lon 213 complete\n",
      "lon 214 complete\n",
      "lon 215 complete\n",
      "lon 216 complete\n",
      "lon 217 complete\n",
      "lon 218 complete\n",
      "lon 219 complete\n",
      "lon 220 complete\n",
      "lon 221 complete\n",
      "lon 222 complete\n",
      "lon 223 complete\n",
      "lon 224 complete\n",
      "lon 225 complete\n",
      "lon 226 complete\n",
      "lon 227 complete\n",
      "lon 228 complete\n",
      "lon 229 complete\n",
      "lon 230 complete\n",
      "lon 231 complete\n",
      "lon 232 complete\n",
      "lon 233 complete\n",
      "lon 234 complete\n",
      "lon 235 complete\n",
      "lon 236 complete\n",
      "lon 237 complete\n",
      "lon 238 complete\n",
      "lon 239 complete\n",
      "lon 240 complete\n",
      "lon 241 complete\n",
      "lon 242 complete\n",
      "lon 243 complete\n",
      "lon 244 complete\n",
      "lon 245 complete\n",
      "lon 246 complete\n",
      "lon 247 complete\n",
      "lon 248 complete\n",
      "lon 249 complete\n",
      "lon 250 complete\n",
      "lon 251 complete\n",
      "lon 252 complete\n",
      "lon 253 complete\n",
      "lon 254 complete\n",
      "lon 255 complete\n",
      "lon 256 complete\n",
      "lon 257 complete\n",
      "lon 258 complete\n",
      "lon 259 complete\n",
      "lon 260 complete\n",
      "lon 261 complete\n",
      "lon 262 complete\n",
      "lon 263 complete\n",
      "lon 264 complete\n",
      "lon 265 complete\n",
      "lon 266 complete\n",
      "lon 267 complete\n",
      "lon 268 complete\n",
      "lon 269 complete\n",
      "lon 270 complete\n",
      "lon 271 complete\n",
      "lon 272 complete\n",
      "lon 273 complete\n",
      "lon 274 complete\n",
      "lon 275 complete\n",
      "lon 276 complete\n",
      "lon 277 complete\n",
      "lon 278 complete\n",
      "lon 279 complete\n",
      "lon 280 complete\n",
      "lon 281 complete\n",
      "lon 282 complete\n",
      "lon 283 complete\n",
      "lon 284 complete\n",
      "lon 285 complete\n",
      "lon 286 complete\n",
      "lon 287 complete\n",
      "lon 288 complete\n",
      "lon 289 complete\n",
      "lon 290 complete\n",
      "lon 291 complete\n",
      "lon 292 complete\n",
      "lon 293 complete\n",
      "lon 294 complete\n",
      "lon 295 complete\n",
      "lon 296 complete\n",
      "lon 297 complete\n",
      "lon 298 complete\n",
      "lon 299 complete\n",
      "lon 300 complete\n",
      "lon 301 complete\n",
      "lon 302 complete\n",
      "lon 303 complete\n",
      "lon 304 complete\n",
      "lon 305 complete\n",
      "lon 306 complete\n",
      "lon 307 complete\n",
      "lon 308 complete\n",
      "lon 309 complete\n",
      "lon 310 complete\n",
      "lon 311 complete\n",
      "lon 312 complete\n",
      "lon 313 complete\n",
      "lon 314 complete\n",
      "lon 315 complete\n",
      "lon 316 complete\n",
      "lon 317 complete\n",
      "lon 318 complete\n",
      "lon 319 complete\n",
      "lon 320 complete\n",
      "lon 321 complete\n",
      "lon 322 complete\n",
      "lon 323 complete\n",
      "lon 324 complete\n",
      "lon 325 complete\n",
      "lon 326 complete\n",
      "lon 327 complete\n",
      "lon 328 complete\n",
      "lon 329 complete\n",
      "lon 330 complete\n",
      "lon 331 complete\n",
      "lon 332 complete\n",
      "lon 333 complete\n",
      "lon 334 complete\n",
      "lon 335 complete\n",
      "lon 336 complete\n",
      "lon 337 complete\n",
      "lon 338 complete\n",
      "lon 339 complete\n",
      "lon 340 complete\n",
      "lon 341 complete\n",
      "lon 342 complete\n",
      "lon 343 complete\n",
      "lon 344 complete\n",
      "lon 345 complete\n",
      "lon 346 complete\n",
      "lon 347 complete\n",
      "lon 348 complete\n",
      "lon 349 complete\n",
      "lon 350 complete\n",
      "lon 351 complete\n",
      "lon 352 complete\n",
      "lon 353 complete\n",
      "lon 354 complete\n",
      "lon 355 complete\n",
      "lon 356 complete\n",
      "lon 357 complete\n",
      "lon 358 complete\n",
      "lon 359 complete\n",
      "lon 360 complete\n",
      "Year 2020 Regrid\n",
      "Variables Saved at path /home/jbassham/north/data/motion_ppv4_latlon_nh_1986_2020.npz\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create arrays for new lat and lon grid\n",
    "    lat = np.arange(LAT_LIM[0], LAT_LIM[1] + YRES) # Latitude\n",
    "    lon = np.arange(LON_LIM[0], LON_LIM[1] + XRES) # Longitude\n",
    "    nlat = len(lat)\n",
    "    nlon = len(lon)\n",
    "    \n",
    "    # define years to process\n",
    "    do_years = np.arange(START_YEAR, END_YEAR + 1)\n",
    "\n",
    "    # Intialize interpolation with one arbitrary grid from the original data\n",
    "    filename = FNAM.format(year = END_YEAR)\n",
    "    path = os.path.join(PATH_SOURCE, filename)\n",
    "\n",
    "    with xr.open_dataset(path) as data:\n",
    "        lat_ease = data['latitude'].values\n",
    "        lon_ease = data['longitude'].values\n",
    "\n",
    "        # NOTE Cropping might not be needed (fast without) - buggy with crop need to also crop when looking for indicies\n",
    "        # # Find indices for crop within limits lat and lon bounds\n",
    "        # # * Extract indices along [0]th dimension\n",
    "        # ix_crop = np.unique(np.where((data['latitude'].values >= LAT_LIM[0]) & (data['latitude'].values <= LAT_LIM[1]) & (data['longitude'].values >= LON_LIM[0]) & (data['longitude'].values <= LON_LIM[1]))[0])\n",
    "        # # * Extract indices along [1]th dimension\n",
    "        # iy_crop = np.unique(np.where((data['latitude'].values >= LAT_LIM[0]) & (data['latitude'].values <= LAT_LIM[1]) & (data['longitude'].values >= LON_LIM[0]) & (data['longitude'].values <= LON_LIM[1]))[1])\n",
    "\n",
    "\n",
    "    # Initialize arrays for interpolation indices\n",
    "    jj = np.zeros((nlat,nlon), dtype=int)\n",
    "    ii = np.zeros((nlat,nlon), dtype=int)\n",
    "\n",
    "    # Iterate through new grid's lat and lon points\n",
    "    for j in range(nlat):\n",
    "        for i in range(nlon):\n",
    "\n",
    "            # Find absolute value distances of j'th lat from entire lat_ease array and store in array\n",
    "            dy = (lat[j]-lat_ease)**2\n",
    "\n",
    "            # Find absolute value distances of i'th lat from entire lat_ease array and store in array  \n",
    "            dx = (lon[i]-lon_ease)**2\n",
    "        \n",
    "            # Find distances (we don't need sqrt here b/c not using actual value, just minimum)\n",
    "            ds = dx + dy\n",
    "\n",
    "            # Find indices of minimum ds value\n",
    "            i_neighbors = np.where(ds == np.min(ds))\n",
    "\n",
    "            # Take minium of lat and lon indices (lower left corner) for consistency, store in array\n",
    "            jj[j,i] = np.min(i_neighbors[0])\n",
    "            ii[j,i] = np.min(i_neighbors[1])\n",
    "\n",
    "    # Initialize lists to append data arrays from each year\n",
    "    u_append = []\n",
    "    v_append = []\n",
    "    error_append = []\n",
    "    t_append = []\n",
    "\n",
    "\n",
    "    # Iterate through years\n",
    "    for year in do_years:\n",
    "        # Create path for each year's file\n",
    "        filename = FNAM.format(year = year)\n",
    "        path = os.path.join(PATH_SOURCE, filename)\n",
    "\n",
    "        # Open dataset as xarray\n",
    "        with xr.open_dataset(path) as data:\n",
    "            # NOTE Cropping might not be needed\n",
    "            # Crop data within bounds \n",
    "            # data = data.isel(x = ix_crop, y = iy_crop)\n",
    "            u_ease = data['u'].values                             # Sea ice x velocity (t, y, x), cm/s\n",
    "            v_ease = data['v'].values                             # Sea ice y velocity (t, y, x), cm/s \n",
    "            error_ease = data['icemotion_error_estimate'].values  # Ice motion error estimates\n",
    "            time = data['time']                                   # time (days since 01,01,1970)\n",
    "            nt = len(time)\n",
    "\n",
    "        # Initialize data arrays for current year\n",
    "        dims = (nt, nlat, nlon)\n",
    "        u = np.zeros(dims) # zonal ice velocity\n",
    "        v = np.zeros(dims) # meridional ice velocity\n",
    "        error = np.zeros(dims) # icemotion error estimates\n",
    "\n",
    "        for i in range(nlon):\n",
    "            for j in range(nlat):\n",
    "                iii = ii[j,i]\n",
    "                jjj = jj[j,i]\n",
    "                \n",
    "                # SOUTHERN HEMISPHERE vector rotation to East/ North\n",
    "                # u[:,j, i] = u_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj, iii])) - v_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj,iii]))\n",
    "                # v[:,j, i] = u_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj,iii]))\n",
    "\n",
    "                # NORTHERN HEMISPHERE vector rotation to East/ North\n",
    "                u[:,j, i] = u_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj,iii]))\n",
    "                v[:,j, i] = -u_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj,iii]))\n",
    "\n",
    "                error[:,j,i] = error_ease[:,jjj,iii]\n",
    "            print(f\"lon {i} complete\")\n",
    "\n",
    "        # Append data to list of years\n",
    "        u_append.append(u)\n",
    "        v_append.append(v)\n",
    "        error_append.append(error)\n",
    "        t_append.append(time)\n",
    "\n",
    "        # Year complete\n",
    "        print(f\"Year {year} Regrid\")\n",
    "\n",
    "    # Concatenate list of yearly data arrays along time dimension\n",
    "    u_concat = np.concatenate(u_append, axis = 0)\n",
    "    v_concat = np.concatenate(v_append, axis = 0)\n",
    "    error_concat = np.concatenate(error_append, axis = 0)\n",
    "\n",
    "    # Save time series data as npz variables\n",
    "    fnam = f\"/motion_ppv4_latlon_nh_{START_YEAR}_{END_YEAR}\"\n",
    "    np.savez_compressed(PATH_DEST + fnam, allow_pickle = True, u = u_concat, v = v_concat, error = error_concat, time = t_append, lat = lat, lon = lon)\n",
    "    print(f\"Variables Saved at path {PATH_DEST + fnam}.npz\")\n",
    "\n",
    "    return  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80fd9db4-4b3c-4018-b8b3-4ce71b8424db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m lat \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m lon \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/seaice/lib/python3.12/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/seaice/lib/python3.12/site-packages/numpy/lib/format.py:795\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/jbassham/north/data/motion_ppv4_latlon_nh_1986_2020.npz')\n",
    "ui = data['u']\n",
    "vi = data['v']\n",
    "errori = data['error']\n",
    "lat = data['lat']\n",
    "lon = data['lon']\n",
    "time = data['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb41e7-2e92-4df6-86ab-d860257a3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving .npz for each year to load in script later ###\n",
    "# def main():\n",
    "# Create arrays for new lat and lon grid\n",
    "lat = np.arange(LAT_LIM[0], LAT_LIM[1] + YRES) # Latitude\n",
    "lon = np.arange(LON_LIM[0], LON_LIM[1] + XRES) # Longitude\n",
    "nlat = len(lat)\n",
    "nlon = len(lon)\n",
    "\n",
    "# define years to process\n",
    "do_years = np.arange(START_YEAR, END_YEAR + 1)\n",
    "\n",
    "# Intialize interpolation with one arbitrary grid from the original data\n",
    "filename = FNAM.format(year = END_YEAR)\n",
    "path = os.path.join(PATH_SOURCE, filename)\n",
    "\n",
    "with xr.open_dataset(path) as data:\n",
    "    lat_ease = data['latitude'].values\n",
    "    lon_ease = data['longitude'].values\n",
    "\n",
    "    # NOTE Cropping might not be needed (fast without) - buggy with crop need to also crop when looking for indicies\n",
    "    # # Find indices for crop within limits lat and lon bounds\n",
    "    # # * Extract indices along [0]th dimension\n",
    "    # ix_crop = np.unique(np.where((data['latitude'].values >= LAT_LIM[0]) & (data['latitude'].values <= LAT_LIM[1]) & (data['longitude'].values >= LON_LIM[0]) & (data['longitude'].values <= LON_LIM[1]))[0])\n",
    "    # # * Extract indices along [1]th dimension\n",
    "    # iy_crop = np.unique(np.where((data['latitude'].values >= LAT_LIM[0]) & (data['latitude'].values <= LAT_LIM[1]) & (data['longitude'].values >= LON_LIM[0]) & (data['longitude'].values <= LON_LIM[1]))[1])\n",
    "\n",
    "\n",
    "# Initialize arrays for interpolation indices\n",
    "jj = np.zeros((nlat,nlon), dtype=int)\n",
    "ii = np.zeros((nlat,nlon), dtype=int)\n",
    "\n",
    "# Iterate through new grid's lat and lon points\n",
    "for j in range(nlat):\n",
    "    for i in range(nlon):\n",
    "\n",
    "        # Find absolute value distances of j'th lat from entire lat_ease array and store in array\n",
    "        dy = (lat[j]-lat_ease)**2\n",
    "\n",
    "        # Find absolute value distances of i'th lat from entire lat_ease array and store in array  \n",
    "        dx = (lon[i]-lon_ease)**2\n",
    "\n",
    "        # Find distances (we don't need sqrt here b/c not using actual value, just minimum)\n",
    "        ds = dx + dy\n",
    "\n",
    "        # Find indices of minimum ds value\n",
    "        i_neighbors = np.where(ds == np.min(ds))\n",
    "\n",
    "        # Take minium of lat and lon indices (lower left corner) for consistency, store in array\n",
    "        jj[j,i] = np.min(i_neighbors[0])\n",
    "        ii[j,i] = np.min(i_neighbors[1])\n",
    "\n",
    "\n",
    "# Iterate through years\n",
    "for year in do_years:\n",
    "    # Create path for each year's file\n",
    "    filename = FNAM.format(year = year)\n",
    "    path = os.path.join(PATH_SOURCE, filename)\n",
    "\n",
    "    # Open dataset as xarray\n",
    "    with xr.open_dataset(path) as data:\n",
    "        # Crop data within bounds \n",
    "#       data = data.isel(x = ix_crop, y = iy_crop)\n",
    "        u_ease = data['u'].values                      # Sea ice x velocity (t, y, x), cm/s\n",
    "        v_ease = data['v'].values                      # Sea ice y velocity (t, y, x), cm/s \n",
    "        error_ease = data['icemotion_error_estimate'].values  # Ice motion error estimates\n",
    "        time = data['time']                            # time (days since 01,01,1970)\n",
    "        nt = len(time)\n",
    "\n",
    "    # Initialize data arrays for current year\n",
    "    dims = (nt, nlat, nlon)\n",
    "    u = np.zeros(dims) # zonal ice velocity\n",
    "    v = np.zeros(dims) # meridional ice velocity\n",
    "    error = np.zeros(dims) # icemotion error estimates\n",
    "\n",
    "    for i in range(nlon):\n",
    "        for j in range(nlat):\n",
    "            iii = ii[j,i]\n",
    "            jjj = jj[j,i]\n",
    "    \n",
    "            # # SOUTHERN HEMISPHERE vector rotation to East/ North\n",
    "            # u[:,j, i] = u_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj, iii])) - v_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj,iii]))\n",
    "            # v[:,j, i] = u_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj,iii]))\n",
    "        \n",
    "            # NORTHERN HEMISPHERE vector rotation to East/ North\n",
    "            u[:,j, i] = u_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj,iii]))\n",
    "            v[:,j, i] = -u_ease[:,jjj, iii]*np.sin(np.radians(lon_ease[jjj, iii])) + v_ease[:,jjj, iii]*np.cos(np.radians(lon_ease[jjj,iii]))\n",
    "\n",
    "            error[:,j,i] = error_ease[:,jjj,iii]\n",
    "            \n",
    "        print(f\"Lon {i} complete\")\n",
    "\n",
    "    # Year complete\n",
    "    print(f\"Year {year} Regrid\")\n",
    "\n",
    "    # Save time series data as npz variables\n",
    "    fnam = f\"motion_ppv4_latlon_nh_{year}\"\n",
    "    np.savez_compressed(os.path.join(PATH_DEST, fnam), u = u, v = v, error = error, time = time, lat = lat, lon = lon)\n",
    "    \n",
    "    print(f\"Variables Saved at path {PATH_DEST + fnam}.npz\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seaice)",
   "language": "python",
   "name": "seaice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
